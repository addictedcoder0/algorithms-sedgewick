# Algorithms, 4th Edition by Robert Sedgewick

---
## 2.3 Quicksort

---
### The Basic Algorithm
The desirable features of quicksort are there it is in-place (uses only a small auxiliary stack) and that it requires time proportional to *N log N* on the average to sort an array of length *N*. The general idea is to rearrange the array such that, when the two subarrays are sorted, the whole array is ordered. This is done by doing the two recursive calls *after* (rather than before for mergesort) working on the whole array, and usin a partition whose position depends on the contents of the array.

#### The partitioning process
The crux of the method is in the partitioning process, which rearranges the array to make the following three conditions hold:  
1. The entry `a[j]` is in its final place in the array, for some `j`
2. No entry in `a[lo]` through `a[j-1]` is greater than `a[j]`
3. No entry in `a[j+1]` through `a[hi]` is less than `a[j]`  

First, we arbitrarily choose `a[lo]` to be the *partioning item* - the one that will go into its final position. Next, we scan from the left end of the array until we find an entry greater than (or equal to) the partitioning item, and we scan from the right end of the array until we find an entry less than (or equal to) the partitioning item. The two items that stopped the scans are out of place in the final partitioned array, so we exchange them. Continuing in this way, we ensure that no array entries to the left of the left index `i` are greater than the partitioning item, and no array entries to the right of the right index `j` are less than the partitioning item. When the scan indices cross, all that we need to do to complete the partitioning process is to exchange the partitioning item `a[lo]` with the rightmost entry of the left subarray (`a[j]`) and return its index `j`.  

Notes: This is a *randomized* algorithm, because it randomly shuffles the array before sorting it. The reason for doing so is to be able to predict (and depend upon) its performance characteristics (e.g. preventing the worst case scenario of a sorted array requiring *N^2* time). It is best to stop the left scan for items with keys *greater than or equal to* the partitioning item's key (and *less than or equal to* for the right scan) even though this creates additional exchanges, because this avoids quadratic running time in certain typical applications.

---
### Implementation Details
- Partitioning in-place: Using an extra array makes partitioning easier (and stable), but it is not worth the cost
- Terminating the loop: Testing whether the pointers cross is a bit trickier than it might seem
- Staying in bounds: The `(j === lo)` test is redundant but the `(i === hi)` test is not
- Preserving randomness: Shuffling is needed for performance guarantee
- Equal keys: When duplicates are present, it is (counter-intuitively) better to stop on keys equal to the partitioning item's key

---
### Performance Characteristics
Quicksort uses *~2N ln N* compares (and one-sixth that many exchanges) on the average to sort an array of length *N* with distinct keys. However, this can be extremely inefficient if the partitions are unbalanced (e.g. if the first partition is on the smallest item, the second partition on the next smallest item, etc.). It uses *~N^2 / 2* compares in the worst case, but random shuffling protects against this case. Quicksort is typically faster in practice than mergesort because it does much less data movement.

---
### Algorithmic Improvements
1. Cutoff to insertion sort - set a cutoff value for subarray size to prevent recursive calls on small subarrays (cutoff values of 5 - 15 works well in most situations)
2. Median-of-three partitioning - Use the median from a sample of size 3 and partition on the middle item. We can further use the sample items as sentinels at the ends of the array and remove both array bounds tests in `partition()`
3. Entropy-optimal sorting - more optimal solutions exist for arrays with large numbers of duplicate keys, see the next section for an implementation with linear-time performance

```js
const fs = require('fs');

/*
 * Partitions the sub-array a[low..high] such that
 *    a[low..j-1] <= a[j] <= a[j+1..high]
 * and return the index j.
 */
const partition = (arr, low, high) => {
  let i = low;
  let j = high + 1;
  let v = arr[low]; // Partitioning element

  while (true) {
    /*
     * Note: the reason we increment i first and decrement j first (and
     * therefore start i at low rather than low+1, and j at high+1 rather than
     * high) is so that we can save the pointer reference to i and j whenever
     * we find two pairs of elements to swap.
     */

    // Find the low item to swap (any item greater than or equal to v)
    while (arr[++i] < v) {
      if (i === high) break;
    }

    // Find the high item to swap (any item less than or equal to v)
    while (arr[--j] > v) {
      if (j === low) break; // Redundant since a[low] acts as sentinel
    }

    // Check if the pointers cross
    if (i >= j) break;

    // Each time we find both i and j, swap them
    [arr[i], arr[j]] = [arr[j], arr[i]];
  }

  /*
   * Swap the positions of the partitioning item v with the element at a[j].
   * This maintains the invariant a[low..j-1] <= a[j] <= a[j+1..high].
   */
  [arr[low], arr[j]] = [arr[j], arr[low]];

  return j;
};

// Implementation of Fischer-Yates Shuffle to guarantee linearithmic performance
const shuffle = (arr) => {
  let counter = arr.length;

  while (counter > 0) {
    // Generate a random integer between 0 and counter
    let rnd = Math.floor(Math.random() * counter);
    counter--;

    // Swap the element at the random index with the element at counter
    [arr[counter], arr[rnd]] = [arr[rnd], arr[counter]];
  }

  return arr;
};

const quicksort = (arr, low=0, high=arr.length-1) => {
  if (low >= high) return;

  if (low === 0 && high === arr.length-1) {
    // If this is the first time running through, shuffle the array
    arr = shuffle(arr);
  }

  // Partition the array on some j, then recursively partition the rest
  let j = partition(arr, low, high);
  quicksort(arr, low, j-1);
  quicksort(arr, j+1, high);

  return arr;
};

```

---
### Quick-select
Useful for finding the *kth* largest item given an array of *N* items (e.g. minimum (k = 0), maximum (k = N-1), median (k = N/2)). Partition the array such that:  
1. Entry `a[j]` is in place
2. No larger entry to the left of `j`
3. No smaller entry to the right of `j`

Repeat in **one** sub-array, depending on `j`, and complete when `j` equals to `k`.  

Analysis: Quick-select takes **linear** time on average.

```js
/*
 * Quick-select implementation. This algorithm rearranges the array so that
 * a[k] contains the kth smallest key, e.g. k = 0 returns the minimum and
 * k = a.length - 1 returns the maximum.
 */

const partition = require('./quicksort').partition;
const shuffle = require('./quicksort').shuffle;

const quickselect = (arr, k) => {
  // Shuffle the array to guarantee average linear-time performance
  arr = shuffle(arr);

  let low = 0;
  let high = arr.length - 1;

  while (low <= high) {
    let j = partition(arr, low, high);

    /*
     * We know that the jth element is sorted such that all elements on its
     * left are less than arr[j] and all elements on its right are greater than
     * arr[j] (or equal to). So in order for us to ensure that the kth element
     * is sorted, we need to partition either the left or right sub-array.
     */
    if (j > k) {
      // Partition left sub-array
      high = j - 1;
    } else if (j < k) {
      // Partition right sub-array
      low = j + 1;
    } else {
      // The kth item is already in position j
      return arr[j];
    }
  }

  /*
   * After sorting a two element sub-array, where low is one less than high, we
   * will increment low or decrement high such that the outer while loop
   * breaks. However, rather than partition an array of size 1, which is
   * trivially partitioned, we know that the element in this position, k, is
   * the kth largest element in the array and we can return it when the while
   * loop breaks at low = high = k.
   */
  return arr[low];
};

```

---
### Quicksort with 3-way partitioning
Partition the array into *three* parts, one each for items with keys smaller than, equal to, and larger than the partitioning item's key. Dijkstra's implementation is based on a single left-to-right pass through the array that maintains a pointer `lt` such that `a[lo..lt-1]` is *less than* `v`, a pointer `gt` such that `a[gt+1..hi]` is *greater than* `v`, and a pointer `i` such that `a[lt..i-1]` are *equal* to `v` and `a[i..gt]` are not yet examined. The three possible cases are handled as follows:  
1. `a[i]` less than `v`: exchange `a[lt]` with `a[i]` and increment both `lt` and `i`
2. `a[i]` greater than `v`: exchange `a[i]` with `a[gt]` and decrement `gt`
3. `a[i]` equal to `v`: increment `i`

#### Algorithm Performance
3-way partitioning speeds up quicksort tremendously in the presence of duplicate keys, and is most effective when there are *few* distinct items. The algorithm is **entropy-optimal** for all distributions of equal keys [Sedgewick-Bentley, 1997], therefore it reduces the running time from linearithmic to linear for a broad class of applications.

```js
const shuffle = require('./quicksort').shuffle;

const quicksort3way = (arr, low=0, high=arr.length-1) => {
  if (low >= high) return;

  let lt = low; // Less than sub-section
  let gt = high; // Greater than sub-section
  let i = low; // Comparison pointer
  let v = arr[low]; // Comparison element, should not change while comparing

  while (i <= gt) {
    if (arr[i] < v) {
      // Element smaller than v -> move into the lt sub-section
      [arr[i], arr[lt]] = [arr[lt], arr[i]];
      i++;
      lt++;
    } else if (arr[i] > v) {
      // Element greater than v -> move into the gt sub-section
      [arr[i], arr[gt]] = [arr[gt], arr[i]];
      gt--;
    } else {
      i++;
    }
  }

  // Sort the less than sub-section
  quicksort3way(arr, low, lt - 1);
  // Sort the greater than sub-section
  quicksort3way(arr, gt + 1, high);

  return arr;
};

```

---
### System Sorts
Native sort implementations typically use `quicksort` for *primitive types* because it is in-place and typically the fastest general-purpose sorting algorithm in practice. Primitive types typically imply that stability is not a concern while performance is. Native sort implementations typically use `mergesort` for *objects* because it is stable and the extra space required is assumed to be not a huge concern.

#### Tukey's ninther
For large arrays, Tukey's ninther algorithm is typically implemented. It uses the median of the median of 3 samples (each of 3 entries). This approximation of the median of 9 enables quicksort to use a maximum of only 12 compares.

---
### Sorting Summary
|             | Inplace? | Stable? | Worst   | Average  | Best    | Remarks                                             |
| ----------- | -------- | ------- | ------- | -------- | ------- | --------------------------------------------------- |
| Selection   | YES      |         | N^2 / 2 | N^2 / 2  | N^2 / 2 | N exchanges                                         |
| Insertion   | YES      | YES     | N^2 / 2 | N^2 / 4  | N       | Use for small N or partially ordered arrays         |
| Shell       | YES      |         | ?       | ?        | N       | Tight code, sub-quadratic                           |
| Merge       |          | YES     | N lg N  | N lg N   | N lg N  | N lg N guarantee, stable                            |
| Quick       | YES      |         | N^2 / 2 | 2N ln N  | N lg N  | N lg N probabilistic guarantee, fastest in practice |
| 3-way Quick | YES      |         | N^2 / 2 | 2N ln N  | N       | Improves quicksort in presence of duplicate keys    |
| Heap        | YES      |         | 2N lg N | 2N lg N  | N lg N  | N lg N guarantee, in place
| ???         | YES      | YES     | N lg N  | N lg N   | N lg N  | Holy sorting grail                                  |