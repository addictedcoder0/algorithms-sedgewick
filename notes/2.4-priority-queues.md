# Algorithms, 4th Edition by Robert Sedgewick

---
## 2.4 Priority Queues

---
### Applications and API
In environments where we need to process the current largest key (but need not key all the keys fully sorted) and be able to support the addition of new keys, we need an environment that supports two operations: *remove the maximum* and *insert*. For example, we could have a stream of a trillion transactions but only want the *M* largest transaction without having the space to store all the transactions. We could implement a min priority queue that removes the minimum once the priority queue exceeds size *M*. The abstract data type **priority queues** can help efficiently support these operations. A priority queue generally follows the following generic API:

```java
public class MaxPQ<Key extends Comparable<Key>>
          MaxPQ()           // create a priority queue
          MaxPQ(int max)    // create a priority queue of initial capacity max
          MaxPQ(Key[] a)    // create a priority queue from the keys in a[]
  void    insert(Key v)     // insert a key into the priority queue
  Key     max()             // return the largest key
  boolean isEmpty()         // is the priority queue empty?
  int     size()            // number of keys in the priority queue
```

---
### Elementary Implementations

1. Array representation (unordered) - `push` keys into an array, exchange the maximum with the last element and `pop` to remove the maximum
2. Array representation (ordered) - move all larger entries to the right when inserting, `pop` the last element to remove the maximum
3. Linked-list representation - modify either the code for `pop()` to find and return the maximum, or the code for `push()` to keep the items in *reverse* order and the code for `pop()` to unlink and return the first (maximum) item on the list  

Using unordered sequences is the prototypical *lazy* approach where we defer doing work until necessary (to find the maximum). Using ordered sequences is the prototypical *eager* approach, doing as much work as we can up front (keeping the list sorted on insertion) to make later operations efficient. However, these data structures have worst-case *linear* time performance, whereas a *heap* data structure enables implementations where *both* operations are guaranteed to be fast.  

| Data structure  | Insert | Remove maximum |
| --------------- | ------ | -------------- |
| Ordered Array   | N      | 1              |
| Unordered Array | 1      | N              |
| Heap            | log N  | log N          |
| (impossible)    | 1      | 1              |

---
### Heap Definitions
In a **binary heap**, the keys are stored in an array such that each key is guaranteed to be larger than (or equal to) the keys at two other specific positions. In turn, each of those keys must be larger than (or equal to) two additional keys, and so forth. We view the keys as being in a binary tree structure with edges from each key to the two keys known to be smaller.  

> A binary tree is *heap-ordered* if the key in each node is larger than or equal to the keys in that node's two children (if any). The largest key in a heap-ordered binary tree is found at the root. A complete tree is one that is perfectly balanced, except for the bottom level.

#### Binary Heap representations
We represent complete binary trees sequentially within an array by putting the node in *level order*, with the root at position 1, its children at positions 2 and 3, their children in positions 4, 5, 6, and 7, and so on.  

> A *binary heap* is a collection of keys arranged in a complete heap-ordered binary tree, represented in level order in an array (not using the first entry).

In a heap, the parent of the node in position *k* is in position *k/2* and, conversely, the two children of the node in position *k* are in positions *2k* and *2k+1*. To move *up* the tree from `a[k]` we set `k` to `k/2`; to move *down* the tree we set `k` to `2*k` or `2*k + 1`. A binary heap guarantees logarithmic-time insert and remove the maximum implementations because the height of a complete binary tree of size *N* is *lg N*.

---
### Algorithms on Heaps
The heap operations that we consider work by first making a simple modification that could violate the heap condition, then traveling through the heap, modifying the heap as required to ensure that the heap condition is satisfied everywhere. We refer to this process as *reheapifying*, or *restoring heap order*. There are two cases. When the priority of some node is increased (or a new node is added at the bottom of a heap), we have to travel *up* the heap to restore the heap order. When the priority of some node is decreased (e.g. if we replace the node at the root with a new node that has a smaller key), we have to travel *down* the heap to restore the heap order.

#### Bottom-up reheapify (swim)
If the heap order is violated because a node's key becomes *larger* than that node's parent's key, then we can make progress toward fixing the violation by exchanging the node with its parent, moving up the heap until we reach a node with a larger key, or the root. The algorithm is written with the knowledge that the parent of the node at position `k` in a heap is at position `k/2`, looping to preserve the invariant that the only place the heap order could be violated is when the node at position `k` might be larger than its parent.

#### Top-down reheapify (sink)
If the heap order is violated because a node's key becomes *smaller* than one or both of that children's keys, then we can make progress toward fixing the violation by exchanging the node with the *larger* of its two children, continuing down the heap until we reach a node with both children smaller (or equal), or the bottom. The algorithm is written with the knowledge that the children of the node at position `k` in a heap are at positions `2k` and `2k + 1`.

The `sink()` and `swim()` operations provide the basis of the following implementations:  
- Insert: add the new key to the end of the array, increment the size of the heap, and then swim up through the heap with that key to restore the heap condition
- Remove the maximum: take the largest item off the top, put the item from the end of the heap at the top, decrement the size of the heap, and then sink down through the heap with that key to restore the heap condition

> In an *N*-key priority queue, the heap algorithms require no more than *1 + lg N* compares for *insert* and no more than *2 lg N* compares for *remove the maximum* (one compare for each node on the path downward).

```js
/*
 * Max Priority Queue implementation using a Binary Heap data structure
 * represented as an array.
 */

class MaxPriorityQueue {
  constructor() {
    this.heap = []; // binary heap that stores the keys
    this.n    = 0;  // number of items in the priority queue
  }

  // Insert a key into the priority queue
  insert(key) {
    /* Add the key to the end of the heap and increment n, then swim up the heap
     * to fix any violations that have arisen.
     */
    this.heap[++this.n] = key;
    this.swim(this.n);
  }

  // Return the largest key from the priority queue
  max() {
    return this.heap[1];
  }

  // Return and remove the largest key from the priority queue
  delMax() {
    /*
     * Save reference to the max key.
     * Swap the max key with the last key in the heap.
     * Decrement n so that the key does not swim back up the heap.
     * Sink down the heap to fix any violations that have arisen.
     * Delete the max key to prevent loitering, and return its reference.
     */
    let max = this.heap[1];

    [this.heap[1], this.heap[this.n]] = [this.heap[this.n], this.heap[1]];
    this.n--;
    this.sink(1);
    this.heap[this.n+1] = null;

    return max;
  }

  // Return the number of items in the priority queue
  size() {
    return this.n;
  }

  // Maintains the heap order by sinking down the heap and fixing violations
  sink(k) {
    while (2*k <= this.n) {
      /*
       * While the comparison node (k) still has children (2k or 2k+1), check
       * the parent against both its children. If it is less than either, swap
       * it with the larger of its children. Continue sinking down the heap
       * until a parent is larger than its two children.
       */
      let parent = this.heap[k];
      let child1 = this.heap[2*k];
      let child2 = this.heap[2*k + 1];

      if (parent < child1 || parent < child2) {
        /*
         * If the parent node is smaller than either of its child nodes, swap
         * with the larger of its two children.
         */
        if (child1 >= child2 || child2 === undefined) {
          [this.heap[k], this.heap[2*k]] = [this.heap[2*k], this.heap[k]];
          k = 2*k;
        } else {
          [this.heap[k], this.heap[2*k+1]] = [this.heap[2*k+1], this.heap[k]];
          k = 2*k + 1;
        }
      } else {
        // Return because the parent node is larger than its two children
        return;
      }
    }
  }

  // Maintains the heap order by swimming up the heap and fixing violations
  swim(k) {
    while (k > 1 && this.heap[Math.floor(k/2)] < this.heap[k]) {
      /*
       * While not at root node, swap k (parent) with k/2 (child) if
       * parent>child. Continue swimming upwards until the invariant holds.
       */
      [this.heap[k], this.heap[Math.floor(k/2)]]
        = [this.heap[Math.floor(k/2)], this.heap[k]];
      k = Math.floor(k / 2);
    }
  }
}

```

#### Multiway heaps
It is not difficult to modify our code to build heaps based on an array representation of complete heap-ordered *ternary* trees, with an entry at position *k* larger than or equal to entries at positions *3k - 1*, *3k*, and *3k + 1* and smaller than or equal to entries at position *(k+1) / 3*, for all indices between 1 and *N* in an array of *N* items, and not much more difficult to use *d*-ary heaps for any given *d*. There is a tradeoff between the lower cost from the reduced tree height (*log_d N)* and the higher cost of finding the largest of the *d* children at each node. This tradeoff is dependent on details of the implementation and expected relative frequency of operations.

#### Index priority queue
We can allow clients to refer to items that are already on the priority queue by associating a unique integer *index* with each item. The following API represents a typical index minimum priority queue:

```java
public class IndexMinPQ<Key extends Comparable<Key>>
           IndexMinPQ(int maxN)       // create a priority queue of capacity maxN
                                      // with possible indices between 0 and maxN - 1
     void  insert(int i, Key key)     // insert key; associate it with index i
     void  changeKey(int i, Key key)  // change the key associated with i to key
  boolean  contains(int i)            // is index i associated with some key?
     void  delete(int i)              // remove i and its associated key
      Key  minKey()                   // return a minimal key
      int  minIndex()                 // return a minimal key's index
      int  delMin()                   // remove a minimal key and return its index
  boolean  isEmpty()                  // is the priority queue empty?
      int  size()                     // number of keys in the priority queue
      Key  keyOf(int i)               // return key associated with index i
```

A useful way iof thinking of this data type is as implementing an array, but with fast access to the smallest entry in the array. It actually does even better, giving fast access to the minimum entry in a *specified subset* of an array's entries (the ones that have been inserted). In other words, you can think of an `IndexMinPQ` named `pq` as representing a subset of an array `pq[0..N-1]` of items. Think of the call `pq.insert(i, key)` as adding `i` to the subset and setting `pq[i] = key` and the call `pq.changeKey(i, key)` as setting `pq[i] = key`, both maintaining the data structures needed to support the other operations, most importantly `delMin()` (remove and return the index of the maximum key) and `changeKey()` (change the item associated with an index that is already in the data structure - just as in `pq[i] = key`). When an item in the heap changes, we can restore the heap invariant with a sink operation (if the key decreases) and a swim operation (if the key increases). To perform the operations, we use the index to find the item in the heap. The abilityu to locate an item in the heap also allows us to add the `delete()` operation to the API.  

> In an index priority queue of size N, the number of companies required is proportional to at most log N for insert, change priority, delete, and remove the minimum.

The `IndexMinPQ` client `Multiway` also solves the *multiway merge* problem: it merges together several sorted input streams into one sorted output stream.  

| Operation     | Worst-case # of compares |
| ------------- | ------------------------ |
| `insert()`    | log N                    |
| `changeKey()` | log N                    |
| `contains()`  | 1                        |
| `delete()`    | log N                    |
| `minKey()`    | 1                        |
| `minIndex()`  | 1                        |
| `delMin()`    | log N                    |

---
### Heapsort
Heapsort breaks into two phases: *heap construction*, where we reorganize the original array into a heap, and the *sortdown*, where we pull teh items out of the heap in decreasing order to build the sorted result. By using `swim()` and `sink()` directly, we can sort an array without needing any extra space, by maintaining the heap within the array to be sorted.

#### Heap construction
To build a heap incrementally would require *N log N* time, however, a more efficient method is to proceed from right to left, using `sink()` to make subheaps as we go. Every position in the array is the root of a small subheap; `sink()` works for such subheaps, as well. If the two children of a node are heaps, then calling `sink()` on that node makes the subtree rooted at the parent a heap. This process establishes the heap order inductively. The scan starts halfway back through the array because we can skip subheaps of size 1. The scan ends at position 1, when we finish building the heap with one call to `sink()`. At the first phase of a sort, heap construction is a bit counterintuitive, because its goal is to produce a heap-ordered result, which ahs the largest item first in the array (and other larger items near the beginning), not at the end, where it is destined to finish.  

> Sink-based heap construction uses fewer than *2N* compares and fewer than *N* exchanges to construct a heap from *N* items.

#### Sortdown
Most of the work during heapsort is done in the second phase, where we remove the largest remaining item from the heap and put it into the array position vacated as the heap shrinks. The process is a bit like selection sort (taking the items in decreasing order instead of in increasing order), but it uses many fewer compares because the heap provides a much more efficient way to find the largest item in the unsorted part of the array.  

> Heapsort uses fewer than *2N lg N + 2N* compares (and half that many exchanges) to sort *N* items.

#### Sink to the bottom, then swim
Most items reinserted into the heap during sortdown go all the way to the bottom, so time can be saved by avoiding the check for whether the item has reached its position, simply promoting the larger of the two children until the bottom is reached, then moving back up the heap to the proper position. This cuts the number of compares by a factor of 2 asymptotically - close to the number used by mergesort (for a randomly-ordered array). It requires additional bookkeping and is useful in practice only when the cost of compares is relatively high (e.g. strings or other types of long keys).  

Overall, heapsort is significant because it is the only method that is optimal (within a constant factor) in its use of both time and space (guaranteed to use *~2N lg N* compares and constant extra space in the worst case. However, it is rarely used in typical applications because it has poor cache performance: array entries are rarely compared with nearby array entries, so the number of cache misses is far higher than for quicksort, mergesort, and even shellsort, where most compares are with nearby entries. Heapsort is also **unstable** because it involves long-distance exchanges.

```js
/*
 * Heapsort implementation that sorts an array in-place in ascending order,
 * uses 0-based indexing.
 */

const sink = (arr, k, n) => {
  /*
   * Maintains the binary heap for arr[k..n] by checking for and fixing
   * violations by swapping smaller parent nodes with the larger of its two
   * child nodes. Uses 0-based indexing.
   */
  while (2*k + 1 < n) {
    /*
     * Note: because 2k + 1 < n rather than 2k + 1 <= n, we are never checking
     * the final sub-heap in order to prevent accidentally swapping back up the
     * recently removed max key. Check if this will produce unexpected results.
     */
    let parent = arr[k];
    let child1 = arr[2*k + 1];
    let child2 = arr[2*k + 2];

    if (parent < child1 || parent < child2) {
      if (child1 >= child2 || child2 === undefined) {
        [arr[k], arr[2*k + 1]] = [arr[2*k + 1], arr[k]];
        k = 2*k + 1;
      } else {
        [arr[k], arr[2*k + 2]] = [arr[2*k + 2], arr[k]];
        k = 2*k + 2;
      }
    } else return;
  }
 };

const heapsort = (arr) => {
  let n = arr.length - 1;

  /*
   * First, build up the heap in-place using the bottom-up method. Go backwards
   * through the heap starting at element at index n/2 (because the rightmost
   * half of the array is composed of little heaps of size 1). Sink each to
   * build out a heap.
   */
  for (let i = Math.floor(n/2); i >= 0; i--) {
    sink(arr, i, n);
  }

  /*
   * Next, sortdown the heap by removing the maximum one at a time, leaving it
   * at the end of the array while decrementing n so that it does not swim back
   * up. Each time a max key is removed, perform a sink operation for the new
   * key at the 0th index, ensuring that the binary heap invariant is restored.
   */
  while (n > 0) {
    [arr[0], arr[n]] = [arr[n], arr[0]];
    n--;
    sink(arr, 0, n);
  }

  return arr;
};

```

